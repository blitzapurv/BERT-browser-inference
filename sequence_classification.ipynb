{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7NzMlqRT-Iu"
      },
      "source": [
        "# Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tifLUkQsT-Iz"
      },
      "source": [
        "Text classification\n",
        "\n",
        "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased)\n",
        "2. Use your finetuned model for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6PvoX_cT-I1"
      },
      "source": [
        "## Load IMDb dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG49ZfDtT-I1"
      },
      "source": [
        "Start by loading the IMDb dataset from the ðŸ¤— Datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8QOOuaFQT-I2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# imdb = load_dataset(\"imdb\")\n",
        "# spam_data = load_dataset(\"TrainingDataPro/email-spam-classification\")\n",
        "# spam_data = load_dataset(\"ucirvine/sms_spam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 80\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 20\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intent_data = DatasetDict.load_from_disk('./data/intent_data')\n",
        "intent_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_labels(example):\n",
        "    if example[\"label\"] == \"irrelevant\":\n",
        "        example[\"label\"] = 0\n",
        "    elif example[\"label\"] == \"relevant\":\n",
        "        example[\"label\"] = 1\n",
        "    return example\n",
        "\n",
        "# spam_data = spam_data.rename_column(\"type\", \"label\")\n",
        "# spam_data = spam_data.remove_columns([\"title\"])\n",
        "# spam_data = spam_data.map(map_labels)\n",
        "intent_data = intent_data.map(map_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 80\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 20\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intent_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsiGT4C4T-I3"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5dSb2v_yT-I3",
        "outputId": "828fc84c-1f01-4c98-8a8c-c6403bf1e984"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'Find the pattern of total network cost for 2023 and percentage change up to 2 decimal place',\n",
              " 'label': 1}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intent_data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK-pPnJFT-I4"
      },
      "source": [
        "There are two fields in this dataset:\n",
        "\n",
        "- `text`: the text.\n",
        "- `label`: a value that is either `0` for a negative review or `1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_nQkzO_T-I5"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Y6_Q8OT-I5"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VYeGJC4xT-I5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/apurva/anaconda3/envs/tfjs_browser/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "# mapping\n",
        "id2label = {0: \"irrelevant\", 1: \"relevant\"}\n",
        "label2id = {\"irrelevant\": 0, \"relevant\": 1}\n",
        "\n",
        "model_name = 'microsoft/xtremedistil-l6-h256-uncased'\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    num_labels=2,\n",
        "    finetuning_task=\"text-classification\",\n",
        "    force_download=False,\n",
        "    trust_remote_code=False,\n",
        "    return_unused_kwargs=False,\n",
        "    id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMcxIeu5T-I6"
      },
      "source": [
        "Create a preprocessing function to tokenize `text` and truncate sequences to be no longer than DistilBERT's maximum input length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xIzrJD_6T-I6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8de265e001664398ae59a19b3a968bc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e92b1add2eb41709cca90b05780b7d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, max_length=512, truncation=True)\n",
        "\n",
        "tokenized_intent_data = intent_data.map(preprocess_function, batched=True)\n",
        "# tokenized_spam_data = spam_data.map(preprocess_function, batched=True)\n",
        "# tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWa7qgOxT-I7"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 80\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 20\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_intent_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QSfrq08T-I7"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsPmXlfTT-I8"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xzLc6HRUT-I8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "import evaluate\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=1)\n",
        "#     return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    precision = precision_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    recall = recall_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'precision': precision['precision'],\n",
        "        'recall': recall['recall'],\n",
        "        'f1': f1['f1'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbjx9pTbT-I8"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XTO_mIUT-I8"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y3pfG-YQT-I9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        "# )\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    # from_tf=,\n",
        "    config=config,\n",
        "    trust_remote_code=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # freeze layers\n",
        "# for param in model.base_model.embeddings.parameters():\n",
        "#     param.requires_grad = False\n",
        "# for param in model.base_model.transformer.layer[:3].parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# trainable_params #total: 66955010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NukMefrtT-I-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/apurva/anaconda3/envs/tfjs_browser/lib/python3.10/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:55, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.497756</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.449155</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.407346</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.370888</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.341970</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.314892</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.292480</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.273308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.255825</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.240962</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.227533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.215735</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.205338</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.196052</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.187998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.180926</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.174782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.169453</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.164929</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.161324</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.158414</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.154636</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.153690</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.153359</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4977564811706543, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0673, 'eval_samples_per_second': 297.241, 'eval_steps_per_second': 44.586, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4491547644138336, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0673, 'eval_samples_per_second': 297.15, 'eval_steps_per_second': 44.573, 'epoch': 2.0}\n",
            "{'eval_loss': 0.40734606981277466, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0772, 'eval_samples_per_second': 259.202, 'eval_steps_per_second': 38.88, 'epoch': 3.0}\n",
            "{'eval_loss': 0.37088775634765625, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0835, 'eval_samples_per_second': 239.5, 'eval_steps_per_second': 35.925, 'epoch': 4.0}\n",
            "{'eval_loss': 0.34196987748146057, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0654, 'eval_samples_per_second': 305.858, 'eval_steps_per_second': 45.879, 'epoch': 5.0}\n",
            "{'eval_loss': 0.3148922026157379, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0675, 'eval_samples_per_second': 296.303, 'eval_steps_per_second': 44.445, 'epoch': 6.0}\n",
            "{'eval_loss': 0.2924800515174866, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0682, 'eval_samples_per_second': 293.455, 'eval_steps_per_second': 44.018, 'epoch': 7.0}\n",
            "{'eval_loss': 0.2733081579208374, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0669, 'eval_samples_per_second': 298.75, 'eval_steps_per_second': 44.813, 'epoch': 8.0}\n",
            "{'eval_loss': 0.2558247148990631, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0681, 'eval_samples_per_second': 293.802, 'eval_steps_per_second': 44.07, 'epoch': 9.0}\n",
            "{'eval_loss': 0.24096167087554932, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0684, 'eval_samples_per_second': 292.568, 'eval_steps_per_second': 43.885, 'epoch': 10.0}\n",
            "{'eval_loss': 0.22753305733203888, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0718, 'eval_samples_per_second': 278.506, 'eval_steps_per_second': 41.776, 'epoch': 11.0}\n",
            "{'eval_loss': 0.21573536098003387, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0701, 'eval_samples_per_second': 285.245, 'eval_steps_per_second': 42.787, 'epoch': 12.0}\n",
            "{'eval_loss': 0.20533844828605652, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0679, 'eval_samples_per_second': 294.463, 'eval_steps_per_second': 44.169, 'epoch': 13.0}\n",
            "{'eval_loss': 0.1960524320602417, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0685, 'eval_samples_per_second': 292.012, 'eval_steps_per_second': 43.802, 'epoch': 14.0}\n",
            "{'eval_loss': 0.1879977285861969, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0655, 'eval_samples_per_second': 305.271, 'eval_steps_per_second': 45.791, 'epoch': 15.0}\n",
            "{'eval_loss': 0.18092641234397888, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0694, 'eval_samples_per_second': 288.357, 'eval_steps_per_second': 43.254, 'epoch': 16.0}\n",
            "{'eval_loss': 0.17478230595588684, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0687, 'eval_samples_per_second': 291.138, 'eval_steps_per_second': 43.671, 'epoch': 17.0}\n",
            "{'eval_loss': 0.16945290565490723, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0687, 'eval_samples_per_second': 291.071, 'eval_steps_per_second': 43.661, 'epoch': 18.0}\n",
            "{'eval_loss': 0.16492947936058044, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0699, 'eval_samples_per_second': 286.137, 'eval_steps_per_second': 42.92, 'epoch': 19.0}\n",
            "{'eval_loss': 0.16132405400276184, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0817, 'eval_samples_per_second': 244.734, 'eval_steps_per_second': 36.71, 'epoch': 20.0}\n",
            "{'eval_loss': 0.15841445326805115, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0724, 'eval_samples_per_second': 276.111, 'eval_steps_per_second': 41.417, 'epoch': 21.0}\n",
            "{'eval_loss': 0.156195729970932, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0666, 'eval_samples_per_second': 300.349, 'eval_steps_per_second': 45.052, 'epoch': 22.0}\n",
            "{'eval_loss': 0.15463586151599884, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0703, 'eval_samples_per_second': 284.674, 'eval_steps_per_second': 42.701, 'epoch': 23.0}\n",
            "{'eval_loss': 0.15368959307670593, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0662, 'eval_samples_per_second': 302.142, 'eval_steps_per_second': 45.321, 'epoch': 24.0}\n",
            "{'eval_loss': 0.1533588469028473, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0756, 'eval_samples_per_second': 264.39, 'eval_steps_per_second': 39.659, 'epoch': 25.0}\n",
            "{'train_runtime': 55.8571, 'train_samples_per_second': 35.806, 'train_steps_per_second': 4.476, 'train_loss': 0.26376043701171875, 'epoch': 25.0}\n"
          ]
        }
      ],
      "source": [
        "class LogCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            # Remove the 'total_flos' log as it is usually not needed\n",
        "            logs.pop(\"total_flos\", None) \n",
        "            if state.is_local_process_zero:\n",
        "                print(logs)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"saved_model/query_intent_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    # save_steps=1000,\n",
        "    save_total_limit=1,  # Keep only the best model and the latest checkpoint\n",
        "\n",
        ")\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_intent_data[\"train\"],\n",
        "    eval_dataset=tokenized_intent_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[LogCallback()]  # Add the custom callback here\n",
        ")\n",
        "\n",
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./saved_model/query_intent_model/best_model/tokenizer_config.json',\n",
              " './saved_model/query_intent_model/best_model/special_tokens_map.json',\n",
              " './saved_model/query_intent_model/best_model/vocab.txt',\n",
              " './saved_model/query_intent_model/best_model/added_tokens.json',\n",
              " './saved_model/query_intent_model/best_model/tokenizer.json')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save best model\n",
        "best_model_dir = \"./saved_model/query_intent_model/best_model\"\n",
        "trainer.model.save_pretrained(best_model_dir, )\n",
        "tokenizer.save_pretrained(best_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51.0M/51.0M [00:13<00:00, 3.81MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/apurvasf/query_intent_model/commit/ee088d9ae948061a7d5ed0b496c2ae58b7b2e96e', commit_message='Upload BertForSequenceClassification', commit_description='', oid='ee088d9ae948061a7d5ed0b496c2ae58b7b2e96e', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# trainer.model.push_to_hub(\"apurvasf/query_intent_model\", token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_runtime': 55.8571,\n",
              " 'train_samples_per_second': 35.806,\n",
              " 'train_steps_per_second': 4.476,\n",
              " 'train_loss': 0.26376043701171875,\n",
              " 'epoch': 25.0}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1533588469028473, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.068, 'eval_samples_per_second': 294.3, 'eval_steps_per_second': 44.145, 'epoch': 25.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.1533588469028473,\n",
              " 'eval_accuracy': 1.0,\n",
              " 'eval_precision': 1.0,\n",
              " 'eval_recall': 1.0,\n",
              " 'eval_f1': 1.0,\n",
              " 'eval_runtime': 0.068,\n",
              " 'eval_samples_per_second': 294.3,\n",
              " 'eval_steps_per_second': 44.145,\n",
              " 'epoch': 25.0}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = trainer.evaluate(eval_dataset=tokenized_intent_data[\"test\"])\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQHeVayuT-I_"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5bkKZYtT-I_"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Grab some text you'd like to run inference on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_dataset = tokenized_intent_data[\"test\"].remove_columns(\"label\")\n",
        "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aDyInuxeT-I_"
      },
      "outputs": [],
      "source": [
        "text1 = \"How is Rupee values against Dollar right now?\"\n",
        "text2 = \"What is the per month overall cost per subs for FY 2023\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'irrelevant', 'score': 0.6782032251358032}] [{'label': 'relevant', 'score': 0.6907285451889038}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"./saved_model/query_intent_model/best_model\")\n",
        "\n",
        "print(classifier(text1), classifier(text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3hUtVSMwT-JG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('relevant', 0.5066925287246704)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model/query_intent_model/best_model\")\n",
        "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
        "\n",
        "# Pass your inputs to the model and return the `logits`\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./saved_model/query_intent_model/best_model\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label\n",
        "import torch.nn.functional as F\n",
        "\n",
        "probabilities = F.softmax(logits, dim=1)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id], probabilities.max().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BertTokenizer, AutoConfig\n",
        "\n",
        "# mapping\n",
        "id2label = {0: \"irrelevant\", 1: \"relevant\"}\n",
        "label2id = {\"irrelevant\": 0, \"relevant\": 1}\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"saved_model/query_intent_model/best_model\",\n",
        "    num_labels=2,\n",
        "    finetuning_task=\"text-classification\",\n",
        "    force_download=False,\n",
        "    trust_remote_code=False,\n",
        "    return_unused_kwargs=False,\n",
        "    id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"saved_model/query_intent_model/best_model\",\n",
        "    use_fast=True,\n",
        "    trust_remote_code=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 2821.67 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 651.65 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, max_length=512, truncation=True)\n",
        "\n",
        "tokenized_intent_data = intent_data.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 1\n",
        "batches_per_epoch = len(tokenized_intent_data[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer, TextClassificationPipeline\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"saved_model/query_intent_model/best_model\",\n",
        "    # from_tf=,\n",
        "    config=config,\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_intent_data[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = model.prepare_tf_dataset(\n",
        "    tokenized_intent_data[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7fa550c6ba30> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7fa550c6ba30> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "20/20 [==============================] - 63s 1s/step - loss: 0.5276 - val_loss: 0.3846 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-12 12:28:59.384346: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7fa4cc2923e0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "model.compile(optimizer=optimizer)  # No loss argument!\n",
        "\n",
        "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
        "callbacks = [metric_callback]\n",
        "\n",
        "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  12750080  \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0 (unused)\n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12750594 (48.64 MB)\n",
            "Trainable params: 12750594 (48.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('relevant', 0.5289333)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model/query_intent_model/best_model\")\n",
        "inputs = tokenizer(text2, return_tensors=\"tf\")\n",
        "\n",
        "logits = model(**inputs).logits\n",
        "\n",
        "probabilities = tf.nn.softmax(logits)\n",
        "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
        "model.config.id2label[predicted_class_id], probabilities.numpy().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/tf_query_intent_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/tf_query_intent_model/assets\n"
          ]
        }
      ],
      "source": [
        "model.save(\"saved_model/tf_query_intent_model\", overwrite=True, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  12750080  \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12750594 (48.64 MB)\n",
            "Trainable params: 12750594 (48.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# from tensorflow.keras.utils import custom_object_scope\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('saved_model/tf_query_intent_model')\n",
        "\n",
        "# Show the model architecture\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensorflow.js format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "del loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "# Convert the PyTorch model to TensorFlow\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"saved_model/query_intent_model/best_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1 = \"How is Rupee values against Dollar right now?\"\n",
        "text2 = \"What is the per month overall cost per subs for FY 2023\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"saved_model/query_intent_model/best_model\")\n",
        "inputs = tokenizer(text2, return_tensors=\"tf\")\n",
        "\n",
        "logits = model(**inputs).logits\n",
        "\n",
        "probabilities = tf.nn.softmax(logits)\n",
        "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
        "model.config.id2label[predicted_class_id], probabilities.numpy().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7fb4bb9f3730>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7fb491a45990>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7fb49172a5f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7fb491728c40>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7fb49173e230>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7fb4917520b0>, because it is not built.\n",
            "INFO:tensorflow:Assets written to: saved_model/tf_query_intent_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/tf_query_intent_model/assets\n"
          ]
        }
      ],
      "source": [
        "# Save the TensorFlow model\n",
        "model.save(\"saved_model/tf_query_intent_model\", overwrite=True)\n",
        "# tokenizer.save_pretrained(\"saved_model/tf_query_intent_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-15 17:43:53.133596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/bin/tensorflowjs_converter\", line 8, in <module>\n",
            "    sys.exit(pip_main())\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/converter.py\", line 959, in pip_main\n",
            "    main([' '.join(sys.argv[1:])])\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/converter.py\", line 963, in main\n",
            "    convert(argv[0].split(' '))\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/converter.py\", line 949, in convert\n",
            "    _dispatch_converter(input_format, output_format, args, quantization_dtype_map,\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/converter.py\", line 655, in _dispatch_converter\n",
            "    tf_saved_model_conversion_v2.convert_tf_saved_model(\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 982, in convert_tf_saved_model\n",
            "    _convert_tf_saved_model(output_dir, saved_model_dir=saved_model_dir,\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 857, in _convert_tf_saved_model\n",
            "    optimized_graph = optimize_graph(frozen_graph, signature,\n",
            "  File \"/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 208, in optimize_graph\n",
            "    raise ValueError('Unsupported Ops in the model after optimization\\n' +\n",
            "ValueError: Unsupported Ops in the model after optimization\n",
            "_MklLayerNorm\n"
          ]
        }
      ],
      "source": [
        "!tensorflowjs_converter \\\n",
        "    --input_format=tf_saved_model \\\n",
        "    --output_format=tfjs_graph_model \\\n",
        "    saved_model/tf_query_intent_model \\\n",
        "    saved_model/tfjs_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-11 15:56:15.765672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: TensorFlow.js model converters. [-h]\n",
            "                                       [--input_format {tf_hub,keras,tf_frozen_model,tfjs_layers_model,keras_keras,keras_saved_model,tf_saved_model}]\n",
            "                                       [--output_format {keras,tfjs_layers_model,keras_keras,keras_saved_model,tfjs_graph_model}]\n",
            "                                       [--signature_name SIGNATURE_NAME]\n",
            "                                       [--saved_model_tags SAVED_MODEL_TAGS]\n",
            "                                       [--quantize_float16 [QUANTIZE_FLOAT16]]\n",
            "                                       [--quantize_uint8 [QUANTIZE_UINT8]]\n",
            "                                       [--quantize_uint16 [QUANTIZE_UINT16]]\n",
            "                                       [--quantization_bytes {1,2}]\n",
            "                                       [--split_weights_by_layer] [--version]\n",
            "                                       [--skip_op_check]\n",
            "                                       [--strip_debug_ops STRIP_DEBUG_OPS]\n",
            "                                       [--use_structured_outputs_names USE_STRUCTURED_OUTPUTS_NAMES]\n",
            "                                       [--weight_shard_size_bytes WEIGHT_SHARD_SIZE_BYTES]\n",
            "                                       [--output_node_names OUTPUT_NODE_NAMES]\n",
            "                                       [--control_flow_v2 CONTROL_FLOW_V2]\n",
            "                                       [--experiments EXPERIMENTS]\n",
            "                                       [--metadata METADATA]\n",
            "                                       [input_path] [output_path]\n",
            "\n",
            "positional arguments:\n",
            "  input_path            Path to the input file or directory. For input format\n",
            "                        \"keras\", an HDF5 (.h5) file is expected. For input\n",
            "                        format \"tensorflow\", a SavedModel directory, frozen\n",
            "                        model file, or TF-Hub module is expected.\n",
            "  output_path           Path for all output artifacts.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --input_format {tf_hub,keras,tf_frozen_model,tfjs_layers_model,keras_keras,keras_saved_model,tf_saved_model}\n",
            "                        Input format. For \"keras\", the input path can be one\n",
            "                        of the two following formats: - A topology+weights\n",
            "                        combined HDF5 (e.g., generated with\n",
            "                        `tf_keras.model.save_model()` method). - A weights-\n",
            "                        only HDF5 (e.g., generated with Keras Model's\n",
            "                        `save_weights()` method). For \"keras_saved_model\", the\n",
            "                        input_path must point to a subfolder under the saved\n",
            "                        model folder that is passed as the argument to\n",
            "                        tf.contrib.save_model.save_keras_model(). The\n",
            "                        subfolder is generated automatically by tensorflow\n",
            "                        when saving keras model in the SavedModel format. It\n",
            "                        is usually named as a Unix epoch time (e.g.,\n",
            "                        1542212752). For \"tf\" formats, a SavedModel, frozen\n",
            "                        model, or TF-Hub module is expected.\n",
            "  --output_format {keras,tfjs_layers_model,keras_keras,keras_saved_model,tfjs_graph_model}\n",
            "                        Output format. Default: tfjs_graph_model.\n",
            "  --signature_name SIGNATURE_NAME\n",
            "                        Signature of the SavedModel Graph or TF-Hub module to\n",
            "                        load. Applicable only if input format is \"tf_hub\" or\n",
            "                        \"tf_saved_model\".\n",
            "  --saved_model_tags SAVED_MODEL_TAGS\n",
            "                        Tags of the MetaGraphDef to load, in comma separated\n",
            "                        string format. Defaults to \"serve\". Applicable only if\n",
            "                        input format is \"tf_saved_model\".\n",
            "  --quantize_float16 [QUANTIZE_FLOAT16]\n",
            "                        Comma separated list of node names to apply float16\n",
            "                        quantization. You can also use wildcard symbol (*) to\n",
            "                        apply quantization to multiple nodes (e.g.,\n",
            "                        conv/*/weights). When the flag is provided without any\n",
            "                        nodes the default behavior will match all nodes.\n",
            "  --quantize_uint8 [QUANTIZE_UINT8]\n",
            "                        Comma separated list of node names to apply 1-byte\n",
            "                        affine quantization. You can also use wildcard symbol\n",
            "                        (*) to apply quantization to multiple nodes (e.g.,\n",
            "                        conv/*/weights). When the flag is provided without any\n",
            "                        nodes the default behavior will match all nodes.\n",
            "  --quantize_uint16 [QUANTIZE_UINT16]\n",
            "                        Comma separated list of node names to apply 2-byte\n",
            "                        affine quantization. You can also use wildcard symbol\n",
            "                        (*) to apply quantization to multiple nodes (e.g.,\n",
            "                        conv/*/weights). When the flag is provided without any\n",
            "                        nodes the default behavior will match all nodes.\n",
            "  --quantization_bytes {1,2}\n",
            "                        (Deprecated) How many bytes to optionally\n",
            "                        quantize/compress the weights to. 1- and 2-byte\n",
            "                        quantizaton is supported. The default (unquantized)\n",
            "                        size is 4 bytes.\n",
            "  --split_weights_by_layer\n",
            "                        Applicable to keras input_format only: Whether the\n",
            "                        weights from different layers are to be stored in\n",
            "                        separate weight groups, corresponding to separate\n",
            "                        binary weight files. Default: False.\n",
            "  --version, -v         Show versions of tensorflowjs and its dependencies\n",
            "  --skip_op_check       Skip op validation for TensorFlow model conversion.\n",
            "  --strip_debug_ops STRIP_DEBUG_OPS\n",
            "                        Strip debug ops (Print, Assert, CheckNumerics) from\n",
            "                        graph.\n",
            "  --use_structured_outputs_names USE_STRUCTURED_OUTPUTS_NAMES\n",
            "                        TFJS graph outputs become a tensor map with the same\n",
            "                        structure as the TF graph structured_outputs (only\n",
            "                        supported for structured_outputs of the form {key1:\n",
            "                        tensor1, key2: tensor2...})\n",
            "  --weight_shard_size_bytes WEIGHT_SHARD_SIZE_BYTES\n",
            "                        Shard size (in bytes) of the weight files. Currently\n",
            "                        applicable only when output_format is\n",
            "                        tfjs_layers_model or tfjs_graph_model.\n",
            "  --output_node_names OUTPUT_NODE_NAMES\n",
            "                        The names of the output nodes, separated by commas.\n",
            "                        E.g., \"logits,activations\". Applicable only if input\n",
            "                        format is \"tf_frozen_model\".\n",
            "  --control_flow_v2 CONTROL_FLOW_V2\n",
            "                        Enable control flow v2 ops, this would improve\n",
            "                        inference performance on models with branches or\n",
            "                        loops.\n",
            "  --experiments EXPERIMENTS\n",
            "                        Enable experimental features, you should only enable\n",
            "                        this flag when using Python3 and TensorFlow nightly\n",
            "                        build.\n",
            "  --metadata METADATA   Attach user defined metadata in format\n",
            "                        key:path/metadata.json Separate multiple metadata\n",
            "                        files by comma.\n"
          ]
        }
      ],
      "source": [
        "!tensorflowjs_converter --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNXRuntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'irrelevant', 'score': 0.9627820253372192}] [{'label': 'relevant', 'score': 0.9815863966941833}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier_pipeline = pipeline(\"text-classification\", model=\"./saved_model/query_intent_model/best_model\")\n",
        "\n",
        "print(classifier_pipeline(text1), classifier_pipeline(text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('irrelevant', 0.9627820253372192)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model/query_intent_model/best_model\")\n",
        "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
        "\n",
        "# Pass your inputs to the model and return the `logits`\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./saved_model/query_intent_model/best_model\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label\n",
        "import torch.nn.functional as F\n",
        "\n",
        "probabilities = F.softmax(logits, dim=1)\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id], probabilities.max().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using framework PyTorch: 2.3.1+cu121\n",
            "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
            "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
            "Found output output_0 with shape: {0: 'batch'}\n",
            "Ensuring inputs are in correct order\n",
            "head_mask is not present in the generated input list.\n",
            "Generated inputs order: ['input_ids', 'attention_mask']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:231: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import transformers.convert_graph_to_onnx as onnx_convert\n",
        "from pathlib import Path\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "onnx_convert.convert_pytorch(classifier_pipeline, opset=11, output=Path(\"saved_model/query_intent_model.onnx\"), use_external_format=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-15 15:45:32.342334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-15 15:45:32.967857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-15 15:45:35.422069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/apurva/anaconda3/envs/clm_ja/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFDistilBertForQuestionAnswering\n",
        "\n",
        "distilbert = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "callable = tf.function(distilbert.call)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "concrete_function = callable.get_concrete_function([tf.TensorSpec([None, 384], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, 384], tf.int32, name=\"attention_mask\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f7396ec7880>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f7396ec7880>, because it is not built.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f73945c24d0>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f73945c24d0>, because it is not built.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f73946111e0>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f73946111e0>, because it is not built.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f7394613eb0>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f7394613eb0>, because it is not built.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f739461ebc0>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f739461ebc0>, because it is not built.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f73946358d0>, because it is not built.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7f73946358d0>, because it is not built.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/distilbert_cased_savedmodel/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/distilbert_cased_savedmodel/assets\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(distilbert, 'saved_model/distilbert_cased_savedmodel', signatures=concrete_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-15 15:53:15.285318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['attention_mask'] tensor_info:\n",
            "      dtype: DT_INT32\n",
            "      shape: (-1, 384)\n",
            "      name: serving_default_attention_mask:0\n",
            "  inputs['input_ids'] tensor_info:\n",
            "      dtype: DT_INT32\n",
            "      shape: (-1, 384)\n",
            "      name: serving_default_input_ids:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['end_logits'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 384)\n",
            "      name: StatefulPartitionedCall:0\n",
            "  outputs['start_logits'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 384)\n",
            "      name: StatefulPartitionedCall:1\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli show --dir saved_model/distilbert_cased_savedmodel --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "clm_ja",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
